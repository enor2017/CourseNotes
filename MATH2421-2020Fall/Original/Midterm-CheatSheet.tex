\documentclass[10pt, a4paper]{article}
\usepackage{geometry}
\geometry{left=0.5in, right=0.5in, top=1in, bottom=1in}
\usepackage{setspace}
\usepackage{amsmath,mathrsfs}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{amsthm}

\newtheorem*{theorem}{Theorem}
\newcommand{\eg}{{\bf {Example: }}}
\newcommand{\sol}{{\bf {Solution: }}}
\renewcommand\qedsymbol{$\blacksquare$}
\newcommand{\E}{{\mathbb{E}}}
\newcommand{\var}{{\rm var}}
\newcommand{\disp}{\displaystyle}

\begin{document}
\begin{spacing}{1}
    \begin{center}
        {\bf MATH2421 Midterm} Cheat Sheet
    \end{center}
    \setlength{\parindent}{0em}
    \parskip=6pt

    {\bf Permutation with objects alike:} $n$ objects of which $n_1$ are alike,
    $n_2$ are alike, $\cdots$, $n_r$ alike: $\dfrac{n!}{n_1!n_2!\cdots n_r!}$

    {\bf Circle arrangement:} $n$ people sitting in a circle, $(n-1)!$ permutations.

    {\bf Properties of combinations:} $\disp {n\choose r}={n-1\choose r-1}+{n-1\choose r}$

    {\bf Binomial Theorem:} $\disp (x+y)^n=\sum_{k=0}^{n}{n\choose k} x^k y^{n-k}$

    {\bf Multinomial Coefficients:} $n$ distinct items divided into $r$ distinct groups of 
    size $n_1,n_2,\cdots,n_r$, respectively, where $\sum n_{i}=n$, then there are 
    $\disp {n\choose n_1,n_2,\cdots,n_r}=\frac{n!}{n_1!n_2!\cdots n_r!}$ arrangements.

    {\bf Multinomial Theorem:} $\disp (x_1+x_2+\cdots +x_r)^n=\sum_{(n_1,\cdots ,n_r):n_1+\cdots +n_r=n}
    {n\choose n_1,n_2,\cdots ,n_r}x_1^{n_1}x_2^{n_2}\cdots x_r^{n_r}$

    \vspace{0.3in}

    {\bf Kolmogorov Axioms:} Probability is a function satisfying: 
    \begin{enumerate}
        \item For any event $A$, $0\le P(A)\le 1$
        \item Let $S$ be the sample space, then $P(S)=1$
        \item For mutually exclusive events $A_1,A_2,\cdots$, there is $\disp P\left(\bigcup_{i=1}^{\infty}A_i\right)
        =\sum_{i=1}^{\infty}P(A_i)$
    \end{enumerate}
    {\bf Inclusion-Exclusion Principle:} 

    $\disp P(A_1\cup A_2\cup \cdots \cup A_n)=
    \sum_{i=1}^{n}P(A_i)-\sum_{i\le i_1<i_2\le n}P(A_{i_1}A_{i_2})+\cdots +
    (-1)^{r+1}\sum_{1\le i_1<\cdots<i_r\le n}P(A_{i_1}\cdots A_{i_r})+\cdots +
    (-1)^{n+1}P(A_1\cdots A_n)$

    \vspace{0.3in}
    {\bf General Multiplication Rule:} $P(A_1A_2\cdots A_n)=
    P(A_1)P(A_2|A_1)P(A_3|A_1A_2)\cdots P(A_n|A_1A_2\cdots A_{n-1})$

    {\bf Total Probability:} $P(B)=P(B|A)P(A)+P(B|A^C)P(A^C)$

    {\bf Bayes' formula:} Events $A_1,\cdots ,A_n$ partitions sample space, assume 
    $P(A_i)>0$ for $1\le i\le n$. Let $B$ be any event, then for any $1\le i\le n$, we have
    $\disp P(A_i|B)=\frac{P(B|A_i)P(A_i)}{P(B|A_1)P(A_1)+\cdots +P(B|A_n)P(A_n)}$

    \vspace{0.3in}
    {\bf Probability mass function:} $\disp p_X(x)=
    \left\{ \begin{array}{ll} 
        P(X=x) & {\rm if\ } x=x_1,x_2,\cdots \\ 0 & {\rm otherwise}
    \end{array} \right.$

    {\bf Cumulative distribution function:} $F_X(x)=P(X\le x)$ for $x\in \mathbb{R}$

    {\bf Expected Value:} $\disp E(X)=\sum_{x} xp_X(x)$, 
    $\disp E[g(x)]=\sum_{i}g(x_i)p_X(x_i)=\sum_x g(x)p_X(x)$

    {\bf Tail Sum Formula:} $\disp E(X)=\sum_{k=1}^{\infty} P(X\ge k)=\sum_{k=0}^{\infty}P(X>k)$

    {\bf Variance:} $\var (X)=E(X-\mu)^2=E(X^2)-[E(X)]^2$

    {\bf Expected Value of Sum of RV:} $\disp E\left[\sum_{i=1}^n X_i\right]
    =\sum_{i=1}^n E[X_i]$

    \newpage
    {\bf Bernoulli random variable:} $Be(p)$, $X=1$ if success, $0$ if failure. 

    $P(X=1)=p, P(X=0)=1-p,\qquad \E(X)=p, \var(X)=p(1-p)$

    {\bf Binomial random variable:} $Bin(n,p)$, $X=$ \# of successes in $n$ 
    Bernoulli($p$) trials.

    For $0\le k\le n$, $\disp P(X=k)={n\choose k} p^k q^{n-k}
    \qquad \E(X)=np, \var(X)=np(1-p)$

    {\bf Geometric random variable:} $Geom(p)$, $X=$ \# of Bernoulli($p$)
    trials required to obtain the first success.

    For $k\ge 1$, $P(X=k)=pq^{k-1}\qquad \E(X)=\dfrac{1}{p}, \var(X)=\dfrac{1-p}{p^2}$.

    {\bf OR, }$X'=$ \# of failures in Bernoulli($p$) trials to obtain 1st success.
    $X=X'+1$

    For $k\ge 0$, $P(X'=k)=pq^k,\qquad \E(X')=\dfrac{1-p}{p}, \var(X')=\dfrac{1-p}{p^2}$

    {\bf Negative Binomial random variable:} $NB(r,p)$, $X=$ \# of Bernoulli($p$) trials
    required to obtain $r$ success.

    For $k\ge r$, $\disp P(X=k)={k-1\choose r-1}p^r q^{k-r}$,
    \qquad $\E(X)=\dfrac{r}{p}, \var(X)=\dfrac{r(1-p)}{p^2}$

    Note that $Geom(p)=NB(1,p)$, $\disp {k-1\choose r-1}=(-1)^{r-1}{-(k-r+1)\choose r-1}$

    {\bf Poisson Random Variable:} $X\sim$ Poisson($\lambda$) 
    \qquad For $k\ge 0$, $\disp P(X=k)=\frac{e^{-\lambda}\lambda^k}{k!},\qquad \E(X)=\lambda, 
    \var(x)=\lambda$

    Usually if $n>20$ and $np<15$, Bin$(n,p)\approx $Poisson($np$).

    {\bf Hypergeometric Random Variable:} $H(n,N,m)$, a set of $N$ balls, 
    of which $m$ are red and $N-m$ are blue. We choose $n$ of these balls 
    {\it without replacement}, $X=$ \# of red balls in sample.

    For $0\le x\le \min(m,n)$, $\disp P(X=x)=\frac{{m\choose x}{N-m\choose n-x}}{{N\choose n}}$
    \qquad $\disp \E(X)=\dfrac{nm}{N}, \var(X)=\frac{nm}{N}
    \left[\frac{(n-1)(m-1)}{N-1}+1-\frac{nm}{N}\right]$


\end{spacing}
\end{document}
